{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90191b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\MOBIT\\AppData\\Local\\Temp\\ipykernel_20344\\809001859.py:1: SyntaxWarning: invalid escape sequence '\\['\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Applying Linear Algebra, Calculus, and Statistics in Machine Learning\\n\\n## Linear Algebra\\n- **Mathematical Model for Linear Regression**:\\n  \\\\[ \\\\hat{y} = X \\\\cdot \\theta \\\\]\\n  \\n  Where:\\n  - X: Feature matrix (with added column of 1s for the bias term)\\n  - θ: Parameter vector (weights and bias)\\n  - ŷ: Predicted values\\n\\n  The feature matrix X has dimensions m×(n+1) where:\\n  - m: Number of training examples\\n  - n: Number of features\\n  - +1: Accounts for the bias term (intercept)\\n\\n## Calculus\\n- **Optimization Objective**:\\n  Minimize the cost function (Mean Squared Error):\\n  \\n  \\\\[ J(\\theta) = \\x0crac{1}{2m} \\\\sum_{i=1}^m (\\\\hat{y}_i - y_i)^2 \\\\]\\n  \\n  Where:\\n  - m: Number of training examples\\n  - ŷ_i: Predicted value for the i-th example\\n  - y_i: Actual value for the i-th example\\n\\n- **Gradient Calculation**:\\n  The gradient of the cost function with respect to θ is:\\n  \\n  \\\\[ \\nabla J(\\theta) = \\x0crac{1}{m} X^T (X \\\\cdot \\theta - y) \\\\]\\n  \\n  This gradient is used in optimization algorithms like gradient descent to update the parameters:\\n  \\n  \\\\[ \\theta := \\theta - \\x07lpha \\nabla J(\\theta) \\\\]\\n  \\n  Where α is the learning rate that controls the step size of each iteration.\\n\\n## Statistics\\n- **Model Evaluation Metrics**:\\n  - Mean Squared Error (MSE): \\n    \\\\[ \\x0crac{1}{m} \\\\sum_{i=1}^m (\\\\hat{y}_i - y_i)^2 \\\\]\\n    Measures the average squared difference between predicted and actual values\\n    \\n  - R-squared (R²):\\n    \\\\[ 1 - \\x0crac{\\\\sum_{i=1}^m (y_i - \\\\hat{y}_i)^2}{\\\\sum_{i=1}^m (y_i - \\x08ar{y})^2} \\\\]\\n    Represents the proportion of variance in the dependent variable that is predictable from the independent variables\\n    \\n  - Additional metrics often used:\\n    * Mean Absolute Error (MAE)\\n    * Root Mean Squared Error (RMSE)\\n    * Adjusted R-squared\\n\\n## Implementation Connection\\nThese mathematical concepts form the foundation for implementing linear regression and many other machine learning algorithms in practice.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Applying Linear Algebra, Calculus, and Statistics in Machine Learning\n",
    "\n",
    "## Linear Algebra\n",
    "- **Mathematical Model for Linear Regression**:\n",
    "  \\[ \\hat{y} = X \\cdot \\theta \\]\n",
    "  \n",
    "  Where:\n",
    "  - X: Feature matrix (with added column of 1s for the bias term)\n",
    "  - θ: Parameter vector (weights and bias)\n",
    "  - ŷ: Predicted values\n",
    "\n",
    "  The feature matrix X has dimensions m×(n+1) where:\n",
    "  - m: Number of training examples\n",
    "  - n: Number of features\n",
    "  - +1: Accounts for the bias term (intercept)\n",
    "\n",
    "## Calculus\n",
    "- **Optimization Objective**:\n",
    "  Minimize the cost function (Mean Squared Error):\n",
    "  \n",
    "  \\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}_i - y_i)^2 \\]\n",
    "  \n",
    "  Where:\n",
    "  - m: Number of training examples\n",
    "  - ŷ_i: Predicted value for the i-th example\n",
    "  - y_i: Actual value for the i-th example\n",
    "\n",
    "- **Gradient Calculation**:\n",
    "  The gradient of the cost function with respect to θ is:\n",
    "  \n",
    "  \\[ \\nabla J(\\theta) = \\frac{1}{m} X^T (X \\cdot \\theta - y) \\]\n",
    "  \n",
    "  This gradient is used in optimization algorithms like gradient descent to update the parameters:\n",
    "  \n",
    "  \\[ \\theta := \\theta - \\alpha \\nabla J(\\theta) \\]\n",
    "  \n",
    "  Where α is the learning rate that controls the step size of each iteration.\n",
    "\n",
    "## Statistics\n",
    "- **Model Evaluation Metrics**:\n",
    "  - Mean Squared Error (MSE): \n",
    "    \\[ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)^2 \\]\n",
    "    Measures the average squared difference between predicted and actual values\n",
    "    \n",
    "  - R-squared (R²):\n",
    "    \\[ 1 - \\frac{\\sum_{i=1}^m (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^m (y_i - \\bar{y})^2} \\]\n",
    "    Represents the proportion of variance in the dependent variable that is predictable from the independent variables\n",
    "    \n",
    "  - Additional metrics often used:\n",
    "    * Mean Absolute Error (MAE)\n",
    "    * Root Mean Squared Error (RMSE)\n",
    "    * Adjusted R-squared\n",
    "\n",
    "## Implementation Connection\n",
    "These mathematical concepts form the foundation for implementing linear regression and many other machine learning algorithms in practice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d9938ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate Synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Add bias term to feature matrix\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "\n",
    "# Initialze parameters\n",
    "\n",
    "theta = np.random.randn(2 ,1)\n",
    "learning_rate = 0.1\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63770bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Implement the Mathematical Formula for Linear Regression\n",
    "\n",
    "def predict(X, theta):\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e80d7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2: Use Gradient Descent to Optimize the Model Parameters\n",
    "def gradient_decent(X, y, theta,learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        gradients = (1/m) * np.dot(X.T, (np.dot(X,theta) - y))\n",
    "        theta -= learning_rate * gradients\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87d30378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3; Calculation Evaluation Metrix\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19e06731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters (Theta):  [[4.21509616]\n",
      " [2.77011339]]\n",
      "MSE:  0.8065845639670531\n",
      "R2:  0.7692735413614223\n"
     ]
    }
   ],
   "source": [
    "theta_optimized = gradient_decent(X_b, y, theta, learning_rate, iterations)\n",
    "y_pred = predict(X_b, theta_optimized)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r_squared(y, y_pred)\n",
    "\n",
    "print(\"Optimized Parameters (Theta): \", theta_optimized)\n",
    "print(\"MSE: \", mse)\n",
    "print(\"R2: \",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68fb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6e17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e2089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05afc0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.74908024, 1.90142861, 1.46398788, 1.19731697, 0.31203728,\n",
       "        0.31198904, 0.11616722, 1.73235229, 1.20223002, 1.41614516,\n",
       "        0.04116899, 1.9398197 , 1.66488528, 0.42467822, 0.36364993,\n",
       "        0.36680902, 0.60848449, 1.04951286, 0.86389004, 0.58245828,\n",
       "        1.22370579, 0.27898772, 0.5842893 , 0.73272369, 0.91213997,\n",
       "        1.57035192, 0.39934756, 1.02846888, 1.18482914, 0.09290083,\n",
       "        1.2150897 , 0.34104825, 0.13010319, 1.89777107, 1.93126407,\n",
       "        1.6167947 , 0.60922754, 0.19534423, 1.36846605, 0.88030499,\n",
       "        0.24407647, 0.99035382, 0.06877704, 1.8186408 , 0.51755996,\n",
       "        1.32504457, 0.62342215, 1.04013604, 1.09342056, 0.36970891,\n",
       "        1.93916926, 1.55026565, 1.87899788, 1.7896547 , 1.19579996,\n",
       "        1.84374847, 0.176985  , 0.39196572, 0.09045458, 0.65066066,\n",
       "        0.77735458, 0.54269806, 1.65747502, 0.71350665, 0.56186902,\n",
       "        1.08539217, 0.28184845, 1.60439396, 0.14910129, 1.97377387,\n",
       "        1.54448954, 0.39743136, 0.01104423, 1.63092286, 1.41371469,\n",
       "        1.45801434, 1.54254069, 0.1480893 , 0.71693146, 0.23173812,\n",
       "        1.72620685, 1.24659625, 0.66179605, 0.1271167 , 0.62196464,\n",
       "        0.65036664, 1.45921236, 1.27511494, 1.77442549, 0.94442985,\n",
       "        0.23918849, 1.42648957, 1.5215701 , 1.1225544 , 1.54193436,\n",
       "        0.98759119, 1.04546566, 0.85508204, 0.05083825, 0.21578285]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7367050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
