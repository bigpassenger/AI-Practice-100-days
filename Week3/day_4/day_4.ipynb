{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definite integral  8/3\n",
      "Indefinite integral  x**3/3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Understanding Integrals and their Applications in ML\n",
    "\n",
    "What Are Integrals?\n",
    "- Mathematical operation that computes the area under a curve\n",
    "- Represents accumulation of quantities over an interval\n",
    "- The definite integral of f(x) from a to b is denoted as: ∫[a,b] f(x) dx\n",
    "\n",
    "Applications in ML:\n",
    "1. Probability Distributions:\n",
    "   - Calculating probabilities from probability density functions\n",
    "   - Normalization of distributions (ensuring total area = 1)\n",
    "   - Computing expected values and moments\n",
    "\n",
    "2. Cost Functions:\n",
    "   - Continuous loss functions in regression problems\n",
    "   - Area under ROC curve (AUC) for classification performance\n",
    "   - Integration in reinforcement learning for cumulative rewards\n",
    "\n",
    "Python Implementation Examples:\n",
    "\"\"\"\n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = x ** 2\n",
    "definite_integral = sp.integrate(f, (x, 0, 2))\n",
    "indefinite_integral = sp.integrate(f, x)\n",
    "print(\"Definite integral \",definite_integral )\n",
    "print(\"Indefinite integral \",indefinite_integral )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a289c7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPractical Considerations for SGD:\\n\\n1. Learning Rate Scheduling:\\n   - Often beneficial to decrease learning rate over time\\n   - Common strategies: step decay, exponential decay, cosine annealing\\n\\n2. Batch Size Selection:\\n   - Smaller batches provide more frequent updates but noisier gradients\\n   - Larger batches provide more stable gradients but slower updates\\n   - Typical batch sizes: 32, 64, 128, 256\\n\\n3. Initialization:\\n   - Proper initialization is crucial for convergence\\n   - He/Xavier initialization often works well with SGD variants\\n\\n4. Regularization:\\n   - SGD has a regularizing effect due to its noisy updates\\n   - Often combined with explicit regularization like L2 weight decay\\n\\nCommon Use Cases:\\n- Training deep neural networks\\n- Large-scale machine learning problems\\n- Online learning scenarios\\n- Non-convex optimization problems\\n\\nIn modern deep learning frameworks, Adam is often the default optimizer due to its\\ngenerally good performance across a wide range of problems.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Optimization Concepts\n",
    "\n",
    "- Local vs. Global Minima  \n",
    "  - Local Minimum: A point where the function value is lower than all nearby points, but not necessarily the lowest overall\n",
    "  - Global Minimum: The point where the function achieves its absolute lowest value across the entire domain\n",
    "\n",
    "- Convex Functions  \n",
    "  - Mathematical definition: f(λx₁ + (1-λ)x₂) ≤ λf(x₁) + (1-λ)f(x₂) for all λ ∈ [0, 1] and all x₁, x₂ in the domain\n",
    "  - This property ensures that any local minimum is also a global minimum\n",
    "  - Convex functions have a bowl-shaped curve with no \"dips\" or multiple minima\n",
    "\n",
    "- Non-Convex Functions in ML  \n",
    "  - Most neural network loss functions are non-convex due to their complex architecture\n",
    "  - Non-convex functions have multiple local minima, saddle points, and complex landscapes\n",
    "  - This makes optimization more challenging but allows modeling complex relationships\n",
    "\n",
    "\n",
    "Key Insights for Machine Learning:\n",
    "1. Convex optimization problems are easier to solve as any local minimum is global\n",
    "2. Most real-world ML problems are non-convex, requiring sophisticated optimization techniques\n",
    "3. Understanding the loss landscape helps in selecting appropriate optimization algorithms\n",
    "4. Techniques like momentum, learning rate schedules, and advanced optimizers (Adam, RMSprop)\n",
    "   help navigate non-convex loss landscapes more effectively\n",
    "\n",
    "Practical Implications:\n",
    "- For convex problems: Gradient descent will find the global optimum\n",
    "- For non-convex problems: Optimization results depend on initialization and optimizer choice\n",
    "- Regularization techniques can help make loss landscapes more favorable for optimization\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Stochastic Gradient Descent (SGD) and Its Variants\n",
    "\n",
    "## What is Stochastic Gradient Descent?\n",
    "- Optimization algorithm that uses random subsets (mini-batches) of the data to compute gradients and update parameters\n",
    "- Unlike batch gradient descent which uses the entire dataset, SGD uses a single random sample or small batches\n",
    "- This makes it much faster and able to handle large datasets that don't fit in memory\n",
    "\n",
    "## Why Use SGD?\n",
    "- Faster convergence for large datasets\n",
    "- Can escape local minima due to the noise in gradient estimation\n",
    "- Requires less memory as it processes data in batches\n",
    "- Well-suited for online learning where data arrives sequentially\n",
    "\n",
    "## Variants of SGD\n",
    "\n",
    "### Mini-Batch SGD\n",
    "- Compromise between batch GD and pure SGD\n",
    "- Uses small random subsets (mini-batches) of the data\n",
    "- Balances computational efficiency with gradient stability\n",
    "\n",
    "### Momentum\n",
    "- Adds a velocity term to parameter updates\n",
    "- Helps accelerate convergence in relevant directions\n",
    "- Reduces oscillations in parameter updates\n",
    "- Formula: v = γv + η∇J(θ); θ = θ - v\n",
    "\n",
    "### Adam Optimizer (Adaptive Moment Estimation)\n",
    "- Combines ideas from both Momentum and RMSProp\n",
    "- Maintains exponentially decaying averages of past gradients and squared gradients\n",
    "- Computes adaptive learning rates for different parameters\n",
    "- Particularly effective for problems with noisy or sparse gradients\n",
    "\n",
    "Python Implementation Examples:\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Practical Considerations for SGD:\n",
    "\n",
    "1. Learning Rate Scheduling:\n",
    "   - Often beneficial to decrease learning rate over time\n",
    "   - Common strategies: step decay, exponential decay, cosine annealing\n",
    "\n",
    "2. Batch Size Selection:\n",
    "   - Smaller batches provide more frequent updates but noisier gradients\n",
    "   - Larger batches provide more stable gradients but slower updates\n",
    "   - Typical batch sizes: 32, 64, 128, 256\n",
    "\n",
    "3. Initialization:\n",
    "   - Proper initialization is crucial for convergence\n",
    "   - He/Xavier initialization often works well with SGD variants\n",
    "\n",
    "4. Regularization:\n",
    "   - SGD has a regularizing effect due to its noisy updates\n",
    "   - Often combined with explicit regularization like L2 weight decay\n",
    "\n",
    "Common Use Cases:\n",
    "- Training deep neural networks\n",
    "- Large-scale machine learning problems\n",
    "- Online learning scenarios\n",
    "- Non-convex optimization problems\n",
    "\n",
    "In modern deep learning frameworks, Adam is often the default optimizer due to its\n",
    "generally good performance across a wide range of problems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6710b4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definite integral  1\n",
      "Indefinite integral  -exp(-x)\n"
     ]
    }
   ],
   "source": [
    "################################### Exercise 1 ###################################\n",
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = sp.exp(-x)\n",
    "\n",
    "definite_integral = sp.integrate(f, (x, 0, sp.oo))\n",
    "indefinite_integral = sp.integrate(f, x)\n",
    "print(\"Definite integral \",definite_integral )\n",
    "print(\"Indefinite integral \",indefinite_integral )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77325e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.16427183]\n",
      " [2.69207144]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Generate some data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Add bias term to X\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "#SGD Implementaion\n",
    "\n",
    "def stochastic_gradient_descent(X, y, theta, learning_rate, n_epochs):\n",
    "    m = len(y)\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):\n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            gradients = 2 * xi.T @ (xi @ theta - yi)\n",
    "            theta -= learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "learning_rate = 0.01\n",
    "n_epochs = 50\n",
    "\n",
    "\n",
    "theta_opt = stochastic_gradient_descent(X_b, y, theta, learning_rate, n_epochs)\n",
    "print(theta_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321ee82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
