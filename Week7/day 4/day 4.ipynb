{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5413df02",
   "metadata": {},
   "source": [
    "# ðŸš€ Overview of XGBoost\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ What is XGBoost?\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an **advanced implementation** of the **Gradient Boosting algorithm**. It is specifically designed for:\n",
    "\n",
    "- **Speed** and **performance**.\n",
    "- Introduces various **enhancements** to make it **faster**, **more efficient**, and capable of **handling complex datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Key Improvements Over Traditional Gradient Boosting\n",
    "\n",
    "### 1. **Speed**\n",
    "- XGBoost is optimized for **faster execution** compared to traditional gradient boosting methods.\n",
    "- It achieves this with advanced techniques like **parallelization** and **hardware optimization**.\n",
    "\n",
    "### 2. **Handling Missing Data**\n",
    "- XGBoost handles missing data effectively by automatically learning the best way to deal with it during the training process.\n",
    "\n",
    "### 3. **Regularization**\n",
    "- It incorporates **L1** (Lasso) and **L2** (Ridge) regularization techniques to prevent **overfitting**.\n",
    "- Regularization improves the generalization power of the model.\n",
    "\n",
    "### 4. **Custom Loss Functions**\n",
    "- Users can define **custom loss functions** to tailor the optimization to specific problems, beyond the default regression or classification losses.\n",
    "\n",
    "### 5. **Tree Pruning**\n",
    "- XGBoost uses **tree pruning** for building trees more efficiently. It prunes trees by looking at **leaf nodes**, allowing for a more balanced structure.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- **XGBoost** is an advanced boosting algorithm that significantly improves on traditional gradient boosting by enhancing **speed**, **regularization**, and **handling complex data**.\n",
    "- It includes features like **custom loss functions** and **tree pruning**, which makes it suitable for a wide variety of machine learning tasks, including large-scale and complex datasets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80dca9",
   "metadata": {},
   "source": [
    "# ðŸ§© Hyperparameters in XGBoost and How to Tune Them\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Key Hyperparameters\n",
    "\n",
    "### 1. **Learning Rate (eta)**  \n",
    "- **Purpose**: Controls the contribution of each tree to the overall model.\n",
    "- **Typical Range**: 0.01 to 0.3  \n",
    "- **Effect**: A lower learning rate typically leads to more trees and a better model, but requires more computation time.  \n",
    "- **Tip**: A lower learning rate often results in better generalization but requires more boosting rounds.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Number of Trees (n_estimators)**  \n",
    "- **Purpose**: Determines the total number of boosting rounds or trees to build.\n",
    "- **Effect**:  \n",
    "  - **Larger values** may improve performance but increase computation time.\n",
    "  - Too many trees can lead to overfitting, so balancing this with the learning rate is essential.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Tree Depth (max_depth)**  \n",
    "- **Purpose**: Limits the depth of individual trees, helping to balance bias and variance.\n",
    "- **Effect**:  \n",
    "  - **Shallow trees** generalize better, but may not capture complex relationships.\n",
    "  - **Deeper trees** may overfit and capture noise in the data.\n",
    "- **Typical Range**: 3-10, with larger depths increasing model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Subsample**  \n",
    "- **Purpose**: Controls the fraction of data used to train each tree.\n",
    "- **Effect**: Helps reduce overfitting by randomly selecting a subset of the data for training each tree.\n",
    "- **Typical Range**: 0.5 to 1.0  \n",
    "- **Tip**: Lower values make the model more robust to overfitting but may lead to underfitting if too small.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Colsample_bytree**  \n",
    "- **Purpose**: Controls the fraction of features used for each tree split.\n",
    "- **Effect**: Helps prevent overfitting by selecting a subset of features for each split.\n",
    "- **Typical Range**: 0.5 to 1.0  \n",
    "- **Tip**: Reducing the fraction can make the model more generalizable by preventing it from relying too heavily on specific features.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Regularization Parameters**  \n",
    "- **lambda (L2 Regularization)**: Controls L2 regularization, helping to prevent overfitting by penalizing large weights.\n",
    "- **alpha (L1 Regularization)**: Controls L1 regularization, helping to enforce sparsity in the feature weights.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary of Hyperparameter Tuning\n",
    "\n",
    "- **Learning Rate (eta)**: Lower rates require more trees for convergence but may improve generalization.\n",
    "- **Number of Trees (n_estimators)**: A higher number of trees can improve performance but increases computation time.\n",
    "- **Tree Depth (max_depth)**: Controls bias-variance tradeoff; shallow trees generalize better.\n",
    "- **Subsample**: Helps reduce overfitting by using a subset of the data for each tree.\n",
    "- **Colsample_bytree**: Prevents overfitting by using only a subset of features for splits.\n",
    "- **Regularization (lambda, alpha)**: Helps reduce overfitting by penalizing overly complex models.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Tuning Tips\n",
    "- Start with a **higher number of trees** and a **lower learning rate**.\n",
    "- Use **cross-validation** to find the optimal balance between regularization and tree complexity.\n",
    "- **Grid search** or **random search** can help identify the best combination of hyperparameters.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1611e21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d965692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Classes: ['malignant' 'benign']\n",
      "0.956140350877193\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learmomg_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best Cross-Validation Accuracy: 0.9736263736263737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOBIT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:13:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"learmomg_rate\", \"user_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"features: {data.feature_names}\")\n",
    "print(f\"Classes: {data.target_names}\")\n",
    "\n",
    "# Convert dataset to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)\n",
    "\n",
    "# Train XGBoost model\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "xgb_model = xgb.train(params,  dtrain, num_boost_round = 100)\n",
    "\n",
    "y_pred = (xgb_model.predict(dtest) > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learmomg_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = XGBClassifier(user_label_encoder = False, eval_metric = 'logloss', random_state = 42)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv = 5, scoring='accuracy', n_jobs = -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9971f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
