{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91920e19",
   "metadata": {},
   "source": [
    "# ⚡ Introduction to LightGBM\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What is LightGBM?\n",
    "\n",
    "**LightGBM (Light Gradient Boosting Machine)** is a high-performance implementation of **Gradient Boosting** developed by Microsoft.  \n",
    "It is designed to efficiently handle **large-scale datasets** and **high-dimensional data** with exceptional **speed** and **accuracy**.\n",
    "\n",
    "LightGBM is widely used in machine learning competitions (like Kaggle) and production systems due to its **scalability, efficiency, and strong predictive power**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Key Features of LightGBM\n",
    "\n",
    "### 1. **Histogram-Based Splitting**\n",
    "- Instead of evaluating every possible split point (as in traditional Gradient Boosting), LightGBM creates **histograms of feature values**.\n",
    "- This approach significantly **reduces computation** and **memory usage**.\n",
    "- The algorithm finds optimal split points **faster** without sacrificing much accuracy.\n",
    "\n",
    "### 2. **Leaf-Wise Tree Growth**\n",
    "- Unlike XGBoost (which grows trees **level-wise**), LightGBM grows trees **leaf-wise**.\n",
    "- It chooses the **leaf with the highest loss reduction** and splits it, leading to **deeper and more complex trees**.\n",
    "- This often improves accuracy, especially on large datasets.\n",
    "\n",
    "### 3. **Support for GPU Training**\n",
    "- LightGBM can leverage **GPU acceleration**, allowing it to handle **massive datasets** and train models much faster than CPU-only algorithms.\n",
    "\n",
    "### 4. **Handling Sparse Data**\n",
    "- Efficiently supports **missing values** and **sparse datasets**, commonly found in text mining or one-hot encoded features.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Advantages of LightGBM\n",
    "\n",
    "- 🚀 **Faster Training:**  \n",
    "  Significantly faster than XGBoost due to its optimized leaf-wise tree growth and histogram-based algorithms.\n",
    "  \n",
    "- 💾 **Memory Efficient:**  \n",
    "  Reduces memory usage with histogram-based splitting, making it ideal for massive datasets.\n",
    "  \n",
    "- 🧮 **Scalable to Large Datasets:**  \n",
    "  Handles millions of data points efficiently, even with thousands of features.\n",
    "  \n",
    "- 🔢 **Highly Accurate:**  \n",
    "  Often achieves better accuracy due to its leaf-wise growth strategy that reduces loss more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 When to Use LightGBM\n",
    "\n",
    "LightGBM is best suited for scenarios that require **high speed and scalability**.\n",
    "\n",
    "✅ **Use LightGBM when:**\n",
    "- You have **large datasets** with mostly **numerical features**.\n",
    "- The problem is **time-sensitive** and requires **fast training** and **prediction**.\n",
    "- You need **high accuracy** without extensive feature engineering.\n",
    "- The dataset contains **sparse features**, **missing values**, or **high-dimensional inputs** (like text or categorical encodings).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Example: Using LightGBM in Python\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load example dataset\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
    "\n",
    "# Create LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred]\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccca89",
   "metadata": {},
   "source": [
    "# 🐱 Overview of CatBoost\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 What is CatBoost?\n",
    "\n",
    "**CatBoost** (short for *Categorical Boosting*) is an advanced **Gradient Boosting** library developed by **Yandex**.  \n",
    "It is specifically designed to handle **categorical features** efficiently, eliminating the need for complex preprocessing steps like **one-hot encoding** or **label encoding**.\n",
    "\n",
    "CatBoost stands out for its simplicity, high accuracy, and robustness, making it an excellent choice for real-world datasets that include a large number of categorical variables.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Key Features of CatBoost\n",
    "\n",
    "### 1. 🧩 **Native Support for Categorical Data**\n",
    "- CatBoost automatically handles categorical features without requiring manual encoding.  \n",
    "- It uses advanced techniques like **ordered target statistics** and **permutation-driven encoding** to convert categories into numeric values in a way that **avoids data leakage** and **reduces overfitting**.\n",
    "\n",
    "### 2. ⚙️ **Ordered Boosting**\n",
    "- Traditional gradient boosting can suffer from overfitting when using target statistics for encoding categories.  \n",
    "- CatBoost introduces **ordered boosting**, a novel algorithm that **ensures the model only uses past data** (not future information) when calculating these statistics — greatly improving generalization.\n",
    "\n",
    "### 3. 🧠 **Robust to Overfitting**\n",
    "- Thanks to its ordered boosting and regularization mechanisms, CatBoost is less prone to overfitting, even with small datasets or many categorical features.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Advantages of CatBoost\n",
    "\n",
    "- 🪄 **No Need for Manual Encoding**  \n",
    "  Automatically handles categorical variables, saving time and preventing preprocessing errors.\n",
    "\n",
    "- 🧩 **Handles Overfitting Gracefully**  \n",
    "  The ordered boosting mechanism reduces the risk of overfitting compared to traditional gradient boosting approaches.\n",
    "\n",
    "- ⚡ **Ease of Use**  \n",
    "  The API is user-friendly and similar to other popular libraries like LightGBM and XGBoost.\n",
    "\n",
    "- 📊 **Strong Performance on Mixed-Type Data**  \n",
    "  Performs exceptionally well on datasets containing both categorical and numerical features.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 When to Use CatBoost\n",
    "\n",
    "✅ **Best suited for:**\n",
    "- Datasets with a **high proportion of categorical features** (e.g., text, location, industry, region, product type, etc.)\n",
    "- **Tabular data** where preprocessing categorical variables is cumbersome.\n",
    "- **Applications prone to overfitting** (due to limited data or noisy labels).\n",
    "- **Data science competitions** and **production systems** needing fast, accurate models.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Example: Using CatBoost in Python\n",
    "\n",
    "```python\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load example data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0baf0",
   "metadata": {},
   "source": [
    "# ⚖️ Comparison of XGBoost, LightGBM, and CatBoost\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Overview\n",
    "\n",
    "While **XGBoost**, **LightGBM**, and **CatBoost** are all powerful implementations of **Gradient Boosting**, they each have different design philosophies, optimization strategies, and best-use scenarios.  \n",
    "Understanding their trade-offs is essential for selecting the right model for your dataset and computational constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Detailed Comparison\n",
    "\n",
    "| **Feature** | **XGBoost** | **LightGBM** | **CatBoost** |\n",
    "|--------------|--------------|---------------|---------------|\n",
    "| **Speed** | Moderate | ⚡ Fast | ⚡ Fast |\n",
    "| **Handling Categorical Data** | ❌ Requires encoding (one-hot or label) | ❌ Requires encoding | ✅ Native support |\n",
    "| **Memory Usage** | Moderate | 🔹 Low | Moderate |\n",
    "| **Tuning Complexity** | Moderate | 🔸 High (due to leaf-wise growth) | ✅ Low |\n",
    "| **Best Use Cases** | General-purpose models | Large datasets | Categorical-heavy datasets |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Explanation by Category\n",
    "\n",
    "### 🚀 1. Speed and Efficiency\n",
    "- **LightGBM** and **CatBoost** are both highly optimized for speed and scalability.  \n",
    "  - **LightGBM** achieves its speed through **histogram-based splitting** and **leaf-wise tree growth**, which reduces computational overhead.  \n",
    "  - **CatBoost** also performs fast due to its efficient gradient computation and internal handling of categorical variables.\n",
    "- **XGBoost** is slightly slower due to level-wise growth and heavier preprocessing requirements.\n",
    "\n",
    "✅ **Winner:** *LightGBM* (fastest on large numeric datasets)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 2. Handling Categorical Features\n",
    "- **XGBoost** and **LightGBM** require manual preprocessing (like label or one-hot encoding).  \n",
    "- **CatBoost** shines here: it has **native categorical feature handling**, using **ordered target encoding** and **permutation-based statistics** to prevent data leakage and reduce overfitting.\n",
    "\n",
    "✅ **Winner:** *CatBoost* (no manual encoding required)\n",
    "\n",
    "---\n",
    "\n",
    "### 💾 3. Memory Usage\n",
    "- **LightGBM** uses **histogram-based algorithms**, which quantize continuous features into discrete bins — reducing memory consumption and improving cache efficiency.  \n",
    "- **XGBoost** is moderately memory-heavy, while **CatBoost** falls in between.\n",
    "\n",
    "✅ **Winner:** *LightGBM* (most memory-efficient)\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ 4. Tuning Complexity\n",
    "- **LightGBM** can achieve very high accuracy but requires **careful hyperparameter tuning** (e.g., `num_leaves`, `min_data_in_leaf`, `feature_fraction`).\n",
    "- **CatBoost** is **plug-and-play**, needing minimal tuning for good results.  \n",
    "- **XGBoost** offers a good balance between flexibility and stability.\n",
    "\n",
    "✅ **Winner:** *CatBoost* (simplest to tune and deploy)\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 5. Best Use Cases\n",
    "| Use Case | Recommended Algorithm |\n",
    "|-----------|------------------------|\n",
    "| **General-purpose tabular data** | XGBoost |\n",
    "| **Large-scale numerical datasets** | LightGBM |\n",
    "| **Categorical-heavy datasets (marketing, banking, text metadata)** | CatBoost |\n",
    "| **Low-latency applications requiring fast inference** | LightGBM or CatBoost |\n",
    "| **Explainable ML or small-scale datasets** | CatBoost |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 Practical Example: When to Choose Each\n",
    "\n",
    "| Scenario | Recommended Model | Rationale |\n",
    "|-----------|-------------------|------------|\n",
    "| You have millions of rows of numeric data and need the fastest training | **LightGBM** | Optimized for speed and memory efficiency |\n",
    "| Your dataset includes many categorical variables (e.g., customer segments, regions, device types) | **CatBoost** | Handles categorical data natively |\n",
    "| You need a well-tested, reliable, general-purpose boosting model | **XGBoost** | Mature library with wide ecosystem support |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "| **Aspect** | **Best Model** | **Why** |\n",
    "|-------------|----------------|----------|\n",
    "| **Speed** | LightGBM | Histogram-based, leaf-wise growth |\n",
    "| **Categorical Data** | CatBoost | Native handling with ordered boosting |\n",
    "| **Memory Efficiency** | LightGBM | Discretized feature bins |\n",
    "| **Ease of Use** | CatBoost | Minimal preprocessing or tuning |\n",
    "| **Balanced Flexibility** | XGBoost | Stable, versatile, and explainable |\n",
    "\n",
    "---\n",
    "\n",
    "### 🏁 Final Takeaway\n",
    "\n",
    "Each gradient boosting library has its niche:\n",
    "\n",
    "- **XGBoost** → Great for balanced, general-purpose modeling.  \n",
    "- **LightGBM** → Perfect for massive, numeric datasets needing fast training.  \n",
    "- **CatBoost** → Ideal for datasets rich in categorical variables and where overfitting control is crucial.\n",
    "\n",
    "> ⚡ **Choose wisely** based on your dataset type, scale, and time constraints — mastering when to use each can save hours of tuning and deliver better predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "306f7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "# Load Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4ace3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ca21dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
    "target = 'Survived'\n",
    "\n",
    "df.fillna({'Age': df['Age'].median()}, inplace=True)\n",
    "df.fillna({'Embarked': df['Embarked'].mode()[0]}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "683b8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {}\n",
    "for col in ['Sex', 'Embarked']:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "161320d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "962faa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 268, number of negative: 444\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 180\n",
      "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.376404 -> initscore=-0.504838\n",
      "[LightGBM] [Info] Start training from score -0.504838\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM Accuracy: 0.8045\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "lgb_pred = lgb_model.predict(x_test)\n",
    "print(f\"LightGBM Accuracy: {accuracy_score(y_test, lgb_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e2b103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 0.8156\n"
     ]
    }
   ],
   "source": [
    "cat_features = ['Pclass', 'Sex', 'Embarked']\n",
    "cat_model = CatBoostClassifier(cat_features = cat_features, verbose = 0)\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "cat_pred = cat_model.predict(x_test)\n",
    "print(f\"CatBoost Accuracy: {accuracy_score(y_test, cat_pred):.4f}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3686c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 0.8156\n"
     ]
    }
   ],
   "source": [
    "cat_model_native = CatBoostClassifier(cat_features=['Sex', 'Embarked'], verbose = 0)\n",
    "cat_model_native.fit(X_train, y_train)\n",
    "\n",
    "cat_preds_native = cat_model_native.predict(x_test)\n",
    "print(f\"CatBoost Accuracy: {accuracy_score(y_test, cat_preds_native):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27933d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23cf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
